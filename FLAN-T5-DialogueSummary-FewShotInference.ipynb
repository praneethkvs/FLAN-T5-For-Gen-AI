{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bcd695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch torchdata\n",
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b778c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee41079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Dialogue SUmmary Dataset\n",
    "#https://huggingface.co/datasets/knkarthick/dialogsum/tree/main\n",
    "train_dataset = pd.read_csv(\"dialogue-sum-dataset/train.csv\")\n",
    "test_dataset = pd.read_csv(\"dialogue-sum-dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "65e97460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f92c5bc99d47e2a47b63ca11791509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc3b6cec3474c0ca75c28d5b675a121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac4769dfea548cca0adba0b777643f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b94a21ff7324ab69b2e8a1e01228427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66db044a9bd4cd4995e2a8f9e3c5c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65131bb9497f4768afd4ce5fd21b6e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46bc65b5c1e245dab9a3242c556deadc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name='google/flan-t5-large'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a2532aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory: google/flan-t5-large\n"
     ]
    }
   ],
   "source": [
    "#TO go to Default Cache directory location\n",
    "#Finder ---> cmd + Shift + G ---> Paste ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c6a0c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  40\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  200\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [40, 200]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i in example_indices:\n",
    "    print(dash_line)\n",
    "    print('Example ', i)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE:')\n",
    "    print(test_dataset.iloc[i]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(test_dataset.iloc[i]['summary'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e401355e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:  #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "\n",
      "summary:  #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "\n",
      "Tokenized Input:  {'input_ids': tensor([[ 1713,   345, 13515,   536,  4663,    10,   363,    97,    19,    34,\n",
      "             6,  3059,    58,  1713,   345, 13515,   357,  4663,    10,  1142,\n",
      "             3,     9,  1962,     5,    94,    31,     7,     3,   324,    12,\n",
      "          4169,    57,    82,  1605,     5,  1713,   345, 13515,   536,  4663,\n",
      "            10,    27,     7,    34,    58,    27,   141,   150,   800,    34,\n",
      "            47,    78,  1480,     5,    27,   398,    36,   326,   230,     5,\n",
      "          1713,   345, 13515,   357,  4663,    10,   363,    31,     7,     8,\n",
      "         23601,    58,  1713,   345, 13515,   536,  4663,    10,    27,   398,\n",
      "          3579,     8,  4169,    18,    17,  9288,    17,    63,  2412,     5,\n",
      "          1713,   345, 13515,   357,  4663,    10,   148,    31,   162,  2500,\n",
      "            13,    97,   780,     5,    37, 14421,  2478,    19,   182,   885,\n",
      "             5,    94,   751,    31,    17,   240,    72,   145,  6786,   676,\n",
      "            12,   129,   132,     5,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])}\n",
      "\n",
      "\n",
      "Encoded Output:  tensor([[   0, 5780,  536,   10,   94,   31,    7,    3,  324,   12, 4169,    5,\n",
      "            1]])\n",
      "\n",
      "\n",
      "Decoded Output:  Person1: It's ten to nine.\n"
     ]
    }
   ],
   "source": [
    "dialogue = test_dataset.iloc[40]['dialogue']\n",
    "print(\"Dialogue: \",dialogue)\n",
    "print(\"\\n\")\n",
    "\n",
    "summary = test_dataset.iloc[40]['summary']\n",
    "print(\"summary: \", summary)\n",
    "print(\"\\n\")\n",
    "\n",
    "inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "print(\"Tokenized Input: \", inputs)\n",
    "print(\"\\n\")\n",
    "\n",
    "encoded_output = model.generate(inputs[\"input_ids\"], max_new_tokens=50)\n",
    "print(\"Encoded Output: \", encoded_output)\n",
    "print(\"\\n\")\n",
    "\n",
    "output = tokenizer.decode(encoded_output[0], skip_special_tokens=True)\n",
    "print(\"Decoded Output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "642bcfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  40\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "Person1: It's ten to nine.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  200\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in example_indices:\n",
    "    dialogue = test_dataset.iloc[i]['dialogue']\n",
    "    summary = test_dataset.iloc[i]['summary']\n",
    "    \n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=50,)[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "09425e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  40\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "What is the issue the person is facing in the below conversation.\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "The person is late for work.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  200\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "What is the issue the person is facing in the below conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I'm not sure what I'd need to upgrade my system to.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Zero Shot Inference with an Instruction Prompt\n",
    "for i in example_indices:\n",
    "    dialogue = test_dataset.iloc[i]['dialogue']\n",
    "    summary = test_dataset.iloc[i]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    # Input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)    \n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4b1370ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "##One Shot Inference\n",
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = test_dataset.iloc[index]['dialogue']\n",
    "        summary = test_dataset.iloc[index]['summary']\n",
    "        \n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    dialogue = test_dataset.iloc[example_index_to_summarize]['dialogue']\n",
    "    \n",
    "    prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "        \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7de34241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8dd52bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ONE SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n"
     ]
    }
   ],
   "source": [
    "summary = test_dataset.iloc[example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2293265e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Yeah. Just pull on this strip. Then peel off the back.\n",
      "#Person2#: You might make a few enemies this way.\n",
      "#Person1#: If they don't think this is fun, they're not meant to be our friends.\n",
      "#Person2#: You mean your friends. I think it's cruel.\n",
      "#Person1#: Yeah. But it's fun. Look at those two ugly old ladies. . . or are they men?\n",
      "#Person2#: Hurry! Get a shot!. . . Hand it over!\n",
      "#Person1#: I knew you'd come around. . .\n",
      "\n",
      "What was going on?\n",
      "#Person1# is about to make a prank. #Person2# thinks it's cruel at first but then joins.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What's wrong with you, Mr. Polly?\n",
      "#Person2#: What's wrong? I want a break from this horrible job.\n",
      "#Person1#: Then, buy a bottle of soft drink.\n",
      "#Person2#: Would you like to buy a bottle for me in the shop?\n",
      "#Person1#: It's a problem, because my boss is in that shop now.\n",
      "#Person2#: Ok, I will go there myself.\n",
      "#Person1#: Sorry, Mr. Polly.\n",
      "#Person2#: It doesn't matter. Oh, God, I have only four dollars in my wallet. Is that possible for me to buy one?\n",
      "#Person1#: Have a try.\n",
      "\n",
      "What was going on?\n",
      "Mr. Polly is tired and wants a break from work. #Person1# cannot buy a bottle of soft drink for him.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Mary, why are you so tired?\n",
      "#Person2#: I go to the personnel market every day, and put a lot of energy into it.\n",
      "#Person1#: Why don't you think about applying for a job on the Internet?\n",
      "#Person2#: On the Internet? I have never tried that way.\n",
      "#Person1#: Well, it's very convenient and very popular now.\n",
      "#Person2#: So how to apply for a job on the Internet?\n",
      "#Person1#: Just get online and send your application through email to the employer.\n",
      "#Person2#: But how can I know which employer is hiring?\n",
      "#Person1#: You could place your job hunting information on the Job Wanted channel on a recruitment website.\n",
      "#Person2#: Will the employers see my information?\n",
      "#Person1#: Yes. If they think you are a fit, maybe they will contact you.\n",
      "#Person2#: Is it safe to job hunt on the Internet?\n",
      "#Person1#: If you log in some formal websites, it must be very safe.\n",
      "#Person2#: I will give it a try at once.\n",
      "\n",
      "What was going on?\n",
      "#Person1# advises Mary to try applying for a job online and teaches her how to do it. Mary will try it.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Few Shot Inference\n",
    "example_indices_full = [50, 90, 220]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "376fcc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "#Person1 recommends upgrading the system and hardware.\n"
     ]
    }
   ],
   "source": [
    "summary = test_dataset.iloc[example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(inputs[\"input_ids\"],max_new_tokens=50,)[0], \n",
    "    skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Generative Configuration Parameters for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ac65b9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "#Person1 recommends upgrading the system and hardware.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#generation_config = GenerationConfig(max_new_tokens=50)\n",
    "#generation_config = GenerationConfig(max_new_tokens=10)\n",
    "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "#generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "#generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(inputs[\"input_ids\"], generation_config=generation_config,)[0], \n",
    "    skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0f94d036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "Person2 is considering upgrading her system. She would like to add a painting program to her software. She needs a faster processor, a more powerful hard disc, more memory and a faster modem. She doesn't have \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=50)\n",
    "#generation_config = GenerationConfig(max_new_tokens=10)\n",
    "#generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "#generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "#generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(inputs[\"input_ids\"], generation_config=generation_config,)[0], \n",
    "    skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61772c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3569222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis Sample\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)       # Show all rows\n",
    "pd.set_option('display.max_colwidth', None)   # Display full column content\n",
    "\n",
    "# Load tokenizer and model using AutoModel\n",
    "model_name = \"google/flan-t5-base\"  # or other Flan-T5 variants\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Function to perform sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    # Prepare input prompt for Flan-T5\n",
    "    input_text = f\"Classify the sentiment of this text: {text}\"\n",
    "    \n",
    "    # Tokenize and encode the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate output\n",
    "    outputs = model.generate(**inputs, max_length=50)\n",
    "    \n",
    "    # Decode and return the result\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "text = \"I love using Vertex AI! It's incredibly powerful and easy to use.\"\n",
    "sentiment = analyze_sentiment(text)\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "\n",
    "#load Sentiment Analysis Dataset\n",
    "df = pd.read_csv(\"sentiment-analysis.csv\")\n",
    "\n",
    "df=df.dropna()\n",
    "newdf=pd.DataFrame(columns = ['Text', 'Sentiment', 'Source','Date/Time', 'User ID', 'Location', 'Confidence Score'])\n",
    "for i in df.index:\n",
    "    aux = df.iloc[i,0].split(\",\")\n",
    "    newdf.loc[len(newdf)] = aux\n",
    "\n",
    "newdf['Text'] = newdf['Text'].str.replace('\"', '')\n",
    "\n",
    "newdf['flan-t5-pred'] = [analyze_sentiment(text) for text in newdf[\"Text\"]]\n",
    "pd.crosstab(newdf[\"Sentiment\"],newdf[\"flan-t5-pred\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
